# -*- coding: utf-8 -*-
"""
Data Ingestion Layer - The Abstraction for Multimodal Input
Designed for extensibility: Today Excel, Tomorrow Images, Voice, etc.
"""
from dataclasses import dataclass
from typing import Iterator, Literal, Any
from abc import ABC, abstractmethod
import pandas as pd
from pathlib import Path


@dataclass
class QuotationRequest:
    """
    ç»Ÿä¸€çš„æŠ¥ä»·è¯·æ±‚æ•°æ®ç»“æ„
    
    è¿™ä¸ªæŠ½è±¡å±‚æ˜¯å…³é”®ï¼šæ— è®ºè¾“å…¥æ¥è‡ªExcelã€å›¾ç‰‡ã€è¯­éŸ³ï¼Œéƒ½è½¬æ¢ä¸ºæ­¤æ ‡å‡†æ ¼å¼
    è¿™æ ·ä¸‹æ¸¸å¤„ç†é€»è¾‘ï¼ˆParserã€Matcherã€Pricerï¼‰å®Œå…¨ä¸éœ€è¦çŸ¥é“æ•°æ®æ¥æº
    """
    source_id: str  # æ•°æ®æ¥æºæ ‡è¯† (e.g., "Row 1", "Screenshot_001.png")
    content: Any  # ä¸»è¦å†…å®¹ (æ–‡æœ¬/å›¾ç‰‡è·¯å¾„/éŸ³é¢‘è·¯å¾„ç­‰)
    content_type: Literal["text", "image", "audio"]  # å†…å®¹ç±»å‹
    context_notes: str = ""  # è¡¥å……å¤‡æ³¨ä¿¡æ¯
    
    # æ–°å¢ï¼šç›´æ¥ä»Excelæå–çš„ç»“æ„åŒ–æ•°æ®
    product_name: str = "ECS"  # äº§å“åç§° (é»˜è®¤ECSäº‘æœåŠ¡å™¨)
    host_count: int = 1  # ä¸»æœºæ•°
    cpu_cores: int = None  # CPUæ ¸å¿ƒæ•°
    memory_gb: int = None  # å†…å­˜(GB)
    storage_gb: int = None  # å­˜å‚¨(GB)
    
    def __str__(self) -> str:
        """ä¾¿äºè°ƒè¯•çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        content_preview = str(self.content)[:50] + "..." if len(str(self.content)) > 50 else str(self.content)
        return f"QuotationRequest(source={self.source_id}, type={self.content_type}, content={content_preview})"


class BaseDataLoader(ABC):
    """
    æ•°æ®åŠ è½½å™¨æŠ½è±¡åŸºç±»
    
    è®¾è®¡ç†å¿µ:
    - å®šä¹‰ç»Ÿä¸€çš„æ¥å£ï¼Œè®©æ‰¹å¤„ç†å™¨ä¸å…·ä½“æ•°æ®æ ¼å¼è§£è€¦
    - ä»Šå¤©å®ç°ExcelDataLoaderï¼Œæ˜å¤©å¯ä»¥å®ç°ImageDirLoaderã€VoiceTranscriptLoader
    - æ‰¹å¤„ç†é€»è¾‘æ— éœ€ä»»ä½•æ”¹åŠ¨
    """
    
    @abstractmethod
    def load_data(self) -> Iterator[QuotationRequest]:
        """
        åŠ è½½æ•°æ®å¹¶è½¬æ¢ä¸ºQuotationRequestæµ
        
        Yields:
            QuotationRequest: æ ‡å‡†åŒ–çš„æŠ¥ä»·è¯·æ±‚å¯¹è±¡
        """
        pass
    
    @abstractmethod
    def get_total_count(self) -> int:
        """
        è·å–æ€»æ•°æ®æ¡æ•°ï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
        
        Returns:
            int: æ•°æ®æ€»æ¡æ•°
        """
        pass


class ExcelDataLoader(BaseDataLoader):
    """
    Excelæ•°æ®åŠ è½½å™¨ - BaseDataLoaderçš„å…·ä½“å®ç°
    
    èŒè´£:
    - è¯»å–Excelæ–‡ä»¶
    - å°†æ¯è¡Œæ•°æ®è½¬æ¢ä¸ºQuotationRequestå¯¹è±¡
    - å¤„ç†åˆ—æ˜ å°„å’Œæ•°æ®æ¸…æ´—
    - æ”¯æŒä¸¤ç§æ¨¡å¼ï¼šç®€å•æ¨¡å¼ï¼ˆSpecificationåˆ—ï¼‰å’Œç»“æ„åŒ–æ¨¡å¼ï¼ˆå¤šåˆ—ï¼‰
    """
    
    def __init__(self, file_path: str, spec_column: str = "Specification", remarks_column: str = "Remarks",
                 structured_mode: bool = False, skip_rows: int = 0):
        """
        åˆå§‹åŒ–ExcelåŠ è½½å™¨
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            spec_column: è§„æ ¼è¯´æ˜åˆ—åï¼ˆç®€å•æ¨¡å¼ï¼‰
            remarks_column: å¤‡æ³¨åˆ—åï¼ˆç®€å•æ¨¡å¼ï¼‰
            structured_mode: æ˜¯å¦ä½¿ç”¨ç»“æ„åŒ–æ¨¡å¼ï¼ˆå¤šåˆ—æ ¼å¼ï¼‰
            skip_rows: è·³è¿‡å‰Nè¡Œ
        """
        self.file_path = Path(file_path)
        self.spec_column = spec_column
        self.remarks_column = remarks_column
        self.structured_mode = structured_mode
        self.skip_rows = skip_rows
        self._df = None
        
        # Validate file exists
        if not self.file_path.exists():
            raise FileNotFoundError(f"Excel file not found: {file_path}")
    
    def _load_dataframe(self) -> pd.DataFrame:
        """å»¶è¿ŸåŠ è½½DataFrame"""
        if self._df is None:
            try:
                # ä½¿ç”¨openpyxlè¯»å–ä»¥æ›´å¥½åœ°å¤„ç†å¤æ‚Excel
                import openpyxl
                
                if self.structured_mode:
                    # ç»“æ„åŒ–æ¨¡å¼ï¼šä½¿ç”¨openpyxlç›´æ¥è¯»å–
                    wb = openpyxl.load_workbook(self.file_path)
                    ws = wb.active
                    
                    # æå–æ‰€æœ‰è¡Œæ•°æ®
                    rows_data = []
                    for row in ws.iter_rows(values_only=True):
                        rows_data.append(row)
                    
                    # è·³è¿‡æŒ‡å®šè¡Œæ•°
                    if self.skip_rows > 0:
                        rows_data = rows_data[self.skip_rows:]
                    
                    # å­˜å‚¨åŸå§‹æ•°æ®ä¾›åç»­å¤„ç†
                    self._raw_rows = rows_data
                    self._df = pd.DataFrame()  # ç»“æ„åŒ–æ¨¡å¼ä¸‹ä¸ä½¿ç”¨DataFrame
                else:
                    # ç®€å•æ¨¡å¼ï¼šä½¿ç”¨pandasè¯»å–
                    self._df = pd.read_excel(self.file_path)
                    
                    # Validate required columns
                    if self.spec_column not in self._df.columns:
                        raise ValueError(f"Column '{self.spec_column}' not found in Excel. Available: {list(self._df.columns)}")
                    
                    # Remarks column is optional
                    if self.remarks_column not in self._df.columns:
                        print(f"âš ï¸  Warning: Column '{self.remarks_column}' not found. Using empty remarks.")
                        self._df[self.remarks_column] = ""
                
            except Exception as e:
                raise Exception(f"Failed to load Excel file: {e}")
        
        return self._df
    
    def load_data(self) -> Iterator[QuotationRequest]:
        """
        ä»ExcelåŠ è½½æ•°æ®å¹¶è½¬æ¢ä¸ºQuotationRequestæµ
        
        Yields:
            QuotationRequest: æ¯è¡Œæ•°æ®å¯¹åº”ä¸€ä¸ªè¯·æ±‚å¯¹è±¡
        """
        self._load_dataframe()
        
        if self.structured_mode:
            # ç»“æ„åŒ–æ¨¡å¼ï¼šå¤„ç†å¤šåˆ—æ ¼å¼
            yield from self._load_structured_data()
        else:
            # ç®€å•æ¨¡å¼ï¼šå¤„ç†Specificationåˆ—
            yield from self._load_simple_data()
    
    def _load_simple_data(self) -> Iterator[QuotationRequest]:
        """ç®€å•æ¨¡å¼ï¼šä»Specificationåˆ—åŠ è½½æ•°æ®"""
        df = self._df
        
        for idx, row in df.iterrows():
            # Extract specification content
            spec_content = str(row[self.spec_column]).strip()
            
            # Skip empty rows
            if not spec_content or spec_content.lower() in ['nan', 'none', '']:
                continue
            
            # Extract remarks (optional)
            remarks = str(row.get(self.remarks_column, "")).strip()
            if remarks.lower() in ['nan', 'none']:
                remarks = ""
            
            # Construct QuotationRequest
            yield QuotationRequest(
                source_id=f"Row {idx + 2}",  # Excel row number (1-indexed + header)
                content=spec_content,
                content_type="text",
                context_notes=remarks
            )
    
    def _load_structured_data(self) -> Iterator[QuotationRequest]:
        """
        ç»“æ„åŒ–æ¨¡å¼ï¼šä»å¤šåˆ—æ ¼å¼åŠ è½½æ•°æ®
        
        é¢„æœŸæ ¼å¼ï¼š
        ç¬¬1è¡Œï¼šæ ‡é¢˜
        ç¬¬2è¡Œï¼šä¸»åˆ—å (ç±»å‹ã€æœåŠ¡å™¨ç±»åˆ«ã€å®‰è£…å†…å®¹ã€è¯´æ˜ã€ä¸»æœºæ•°ã€è™šæ‹Ÿæœºè§„æ ¼)
        ç¬¬3è¡Œï¼šCPU(æ ¸æ•°)ã€å†…å­˜(G)ã€æ•°æ®ç›˜(G)
        ç¬¬4è¡ŒåŠä»¥åï¼šæ•°æ®è¡Œ
        """
        if not hasattr(self, '_raw_rows') or not self._raw_rows:
            return
        
        # æ‰¾åˆ°åŒ…å«CPUã€å†…å­˜ã€å­˜å‚¨çš„åˆ—ç´¢å¼•
        # æ ¹æ®å®é™…æ•°æ®ï¼Œè¿™äº›åˆ—é€šå¸¸åœ¨ç¬¬5-7åˆ—
        cpu_col_idx = 5  # CPU(æ ¸æ•°)
        mem_col_idx = 6  # å†…å­˜(G)
        storage_col_idx = 7  # æ•°æ®ç›˜(G)
        host_count_col_idx = 4  # ä¸»æœºæ•°
        desc_col_idx = 2  # å®‰è£…å†…å®¹/è¯´æ˜
        
        # ä»ç¬¬4è¡Œå¼€å§‹è¯»å–æ•°æ®ï¼ˆç¬¬1è¡Œæ˜¯æ ‡é¢˜ï¼Œç¬¬2-3è¡Œæ˜¯è¡¨å¤´ï¼‰
        for row_idx, row in enumerate(self._raw_rows[3:], start=4):
            try:
                # æå–æ•°æ®ï¼Œç¡®ä¿ä¸è¶…è¿‡è¡Œé•¿åº¦
                if len(row) <= max(cpu_col_idx, mem_col_idx, storage_col_idx):
                    continue
                
                # æå–CPUã€å†…å­˜ã€å­˜å‚¨
                cpu_value = row[cpu_col_idx] if cpu_col_idx < len(row) else None
                mem_value = row[mem_col_idx] if mem_col_idx < len(row) else None
                storage_value = row[storage_col_idx] if storage_col_idx < len(row) else None
                host_count_value = row[host_count_col_idx] if host_count_col_idx < len(row) else None
                desc_value = row[desc_col_idx] if desc_col_idx < len(row) else None
                
                # è·³è¿‡ç©ºè¡Œæˆ–éCPUæ•°æ®è¡Œ
                if cpu_value is None or mem_value is None:
                    continue
                
                # è½¬æ¢ä¸ºæ•´æ•°
                try:
                    cpu_cores = int(cpu_value) if cpu_value else None
                    memory_gb = int(mem_value) if mem_value else None
                    
                    # å­˜å‚¨å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—
                    if storage_value:
                        try:
                            storage_gb = int(storage_value)
                        except (ValueError, TypeError):
                            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚"500"ï¼‰ï¼Œå°è¯•è½¬æ¢
                            storage_str = str(storage_value).strip().replace(',', '')
                            storage_gb = int(storage_str) if storage_str.isdigit() else 0
                    else:
                        storage_gb = 0
                    
                    # ä¸»æœºæ•°
                    if host_count_value:
                        try:
                            # å¤„ç†"å°"ç­‰å•ä½
                            host_count_str = str(host_count_value).replace('å°', '').strip()
                            host_count = int(host_count_str) if host_count_str.isdigit() else 1
                        except (ValueError, TypeError):
                            host_count = 1
                    else:
                        host_count = 1
                    
                    if cpu_cores is None or memory_gb is None:
                        continue
                    
                except (ValueError, TypeError):
                    # æ— æ³•è½¬æ¢ä¸ºæ•°å­—ï¼Œè·³è¿‡
                    continue
                
                # æ„é€ æè¿°æ–‡æœ¬
                desc_text = str(desc_value) if desc_value else ""
                content_text = f"{cpu_cores}C {memory_gb}G"
                if storage_gb > 0:
                    content_text += f" {storage_gb}Gå­˜å‚¨"
                if desc_text:
                    content_text += f" | {desc_text}"
                
                # æ„é€ QuotationRequest
                yield QuotationRequest(
                    source_id=f"Row {row_idx + 1}",  # ExcelåŸå§‹è¡Œå·
                    content=content_text,
                    content_type="text",
                    context_notes=desc_text,
                    host_count=host_count,
                    cpu_cores=cpu_cores,
                    memory_gb=memory_gb,
                    storage_gb=storage_gb
                )
                
            except Exception as e:
                # è·³è¿‡å‡ºé”™çš„è¡Œ
                print(f"âš ï¸  Warning: Failed to parse row {row_idx + 1}: {e}")
                continue
    
    def get_total_count(self) -> int:
        """è·å–æœ‰æ•ˆæ•°æ®è¡Œæ•°"""
        self._load_dataframe()
        
        if self.structured_mode:
            # ç»“æ„åŒ–æ¨¡å¼ï¼šè®¡ç®—æœ‰CPUæ•°æ®çš„è¡Œæ•°
            if not hasattr(self, '_raw_rows') or not self._raw_rows:
                return 0
            
            count = 0
            cpu_col_idx = 5
            mem_col_idx = 6
            
            for row in self._raw_rows[3:]:  # è·³è¿‡è¡¨å¤´
                if len(row) > max(cpu_col_idx, mem_col_idx):
                    cpu_value = row[cpu_col_idx]
                    mem_value = row[mem_col_idx]
                    if cpu_value is not None and mem_value is not None:
                        try:
                            int(cpu_value)
                            int(mem_value)
                            count += 1
                        except (ValueError, TypeError):
                            pass
            return count
        else:
            # ç®€å•æ¨¡å¼ï¼šè®¡ç®—éç©ºè¡Œ
            df = self._df
            valid_rows = df[self.spec_column].notna() & (df[self.spec_column] != "")
            return valid_rows.sum()


# ============================================================================
# LLM-Driven Excel Parser (Phase 7: Intelligent Adaptive Parsing)
# ============================================================================

class LLMDrivenExcelLoader(BaseDataLoader):
    """
    LLMé©±åŠ¨çš„Excelæ•°æ®åŠ è½½å™¨ - æ™ºèƒ½è‡ªé€‚åº”è§£æ
    
    æ ¸å¿ƒç†å¿µ:
    - ä¸ä¾èµ–å›ºå®šçš„åˆ—ç´¢å¼•æˆ–è¡¨æ ¼ç»“æ„
    - è¯»å–æ•´ä¸ªExcelè¡¨æ ¼ï¼Œæå–æ‰€æœ‰æœ‰ç”¨ä¿¡æ¯
    - ä½¿ç”¨Qwen-Plus LLMæ™ºèƒ½ç†è§£å’Œç»“æ„åŒ–æ•°æ®
    - é€‚åº”å„ç§ä¸åŒæ ¼å¼çš„æŠ¥ä»·å•
    
    å·¥ä½œæµç¨‹:
    1. è¯»å–ExcelåŸå§‹æ•°æ®ï¼ˆæ‰€æœ‰è¡Œåˆ—ï¼‰
    2. æå–åŠç»“æ„åŒ–ä¿¡æ¯ï¼ˆæ•°å­—ã€æ–‡æœ¬ã€ä½ç½®å…³ç³»ï¼‰
    3. æ„é€ Promptæäº¤ç»™Qwen-Plus
    4. LLMè¿”å›æ ‡å‡†åŒ–çš„JSONç»“æ„æ•°æ®
    """
    
    def __init__(self, file_path: str, api_key: str = None):
        """
        åˆå§‹åŒ–LLMé©±åŠ¨çš„ExcelåŠ è½½å™¨
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            api_key: DashScope API Keyï¼ˆå¦‚æœä¸æä¾›ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        """
        self.file_path = Path(file_path)
        self.api_key = api_key
        self._raw_rows = None
        self._parsed_data = None
        
        # Validate file exists
        if not self.file_path.exists():
            raise FileNotFoundError(f"Excel file not found: {file_path}")
        
        # Setup API key
        if not self.api_key:
            import os
            from dotenv import load_dotenv
            load_dotenv()
            self.api_key = os.getenv('DASHSCOPE_API_KEY')
            
        if not self.api_key:
            raise ValueError("DASHSCOPE_API_KEY is required for LLM-driven parsing")
    
    def _extract_semi_structured_data(self, sheet_name: str = None) -> str:
        """
        ä»Excelæå–åŠç»“æ„åŒ–æ•°æ®
        
        Args:
            sheet_name: æŒ‡å®šè¦è¯»å–çš„å·¥ä½œè¡¨åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è¯»å–æ´»åŠ¨å·¥ä½œè¡¨
        
        Returns:
            str: æ ¼å¼åŒ–çš„åŠç»“æ„åŒ–æ–‡æœ¬ï¼ŒåŒ…å«æ‰€æœ‰æœ‰ç”¨ä¿¡æ¯
        """
        import openpyxl
        
        # ä½¿ç”¨ data_only=True è¯»å–å…¬å¼çš„è®¡ç®—ç»“æœï¼Œè€Œä¸æ˜¯å…¬å¼æœ¬èº«
        wb = openpyxl.load_workbook(self.file_path, data_only=True)
        ws = wb[sheet_name] if sheet_name else wb.active
        
        # æå–æ‰€æœ‰éç©ºè¡Œæ•°æ®
        rows_data = []
        for row in ws.iter_rows(values_only=True):
            # è¿‡æ»¤æ‰å®Œå…¨ç©ºçš„è¡Œ
            non_empty_cells = [cell for cell in row if cell is not None]
            if non_empty_cells:
                rows_data.append(row)
        
        self._raw_rows = rows_data
        
        # æ„é€ åŠç»“æ„åŒ–æ–‡æœ¬è¡¨ç¤º
        semi_structured_text = "Excelè¡¨æ ¼æ•°æ®ï¼š\n\n"
        
        for row_idx, row in enumerate(rows_data, 1):
            # åªä¿ç•™éç©ºå•å…ƒæ ¼
            row_content = []
            for col_idx, cell in enumerate(row):
                if cell is not None:
                    # è¯†åˆ«æ•°å­—å’Œæ–‡æœ¬
                    cell_str = str(cell).strip()
                    if cell_str:
                        row_content.append(f"[åˆ—{col_idx+1}]{cell_str}")
            
            if row_content:
                semi_structured_text += f"ç¬¬{row_idx}è¡Œ: {' | '.join(row_content)}\n"
        
        return semi_structured_text
    
    def _parse_with_llm(self, semi_structured_data: str) -> list:
        """
        ä½¿ç”¨Qwen-Plus LLMè§£æåŠç»“æ„åŒ–æ•°æ®
        
        Args:
            semi_structured_data: åŠç»“æ„åŒ–æ–‡æœ¬æ•°æ®
            
        Returns:
            list: æ ‡å‡†åŒ–çš„èµ„æºéœ€æ±‚åˆ—è¡¨
        """
        from http import HTTPStatus
        import dashscope
        import json
        
        # è®¾ç½®API Key
        dashscope.api_key = self.api_key
        
        # æ„é€ Prompt
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº‘èµ„æºæŠ¥ä»·å•è§£æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»Excelè¡¨æ ¼æ•°æ®ä¸­æå–äº‘æœåŠ¡å™¨èµ„æºé…ç½®ä¿¡æ¯ã€‚

è¯·ä»”ç»†åˆ†æè¡¨æ ¼æ•°æ®ï¼Œè¯†åˆ«å‡ºæ¯ä¸€ä¸ªèµ„æºé…ç½®é¡¹ï¼Œå¹¶æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
- äº§å“åç§°ï¼ˆå¦‚ï¼šECSäº‘æœåŠ¡å™¨ã€PolarDBæ•°æ®åº“ã€WAFé˜²ç«å¢™ã€äº‘å®‰å…¨ä¸­å¿ƒç­‰ï¼Œé»˜è®¤ä¸º"ECS"ï¼‰
- CPUæ ¸å¿ƒæ•°ï¼ˆæ•´æ•°ï¼‰
- å†…å­˜å¤§å°GBï¼ˆæ•´æ•°ï¼‰
- å­˜å‚¨å¤§å°GBï¼ˆæ•´æ•°ï¼Œå¦‚æœæ²¡æœ‰æ˜ç¡®è¯´æ˜åˆ™ä¸º0ï¼‰
- ä¸»æœºæ•°é‡ï¼ˆæ•´æ•°ï¼Œé»˜è®¤1ï¼‰
- èµ„æºæè¿°ï¼ˆç®€çŸ­æè¿°è¿™æ˜¯ä»€ä¹ˆæœåŠ¡æˆ–ç”¨é€”ï¼‰

è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
{
  "row_number": è¡Œå·,
  "product_name": "äº§å“åç§°",
  "cpu_cores": CPUæ ¸å¿ƒæ•°,
  "memory_gb": å†…å­˜GB,
  "storage_gb": å­˜å‚¨GB,
  "host_count": ä¸»æœºæ•°,
  "description": "èµ„æºæè¿°"
}

æ³¨æ„äº‹é¡¹ï¼š
1. åªæå–å®é™…çš„èµ„æºé…ç½®æ•°æ®è¡Œï¼Œå¿½ç•¥æ ‡é¢˜ã€è¡¨å¤´ã€ç©ºè¡Œ
2. CPUå’Œå†…å­˜å¿…é¡»æ˜¯æœ‰æ•ˆçš„æ­£æ•´æ•°
3. äº§å“åç§°è¯†åˆ«è§„åˆ™ï¼š
   - å¦‚æœæåˆ°"æ•°æ®åº“"ã€"MySQL"ã€"PolarDB"ã€"RDS" -> "PolarDB"
   - å¦‚æœæåˆ°"é˜²ç«å¢™"ã€"WAF" -> "WAF"
   - å¦‚æœæåˆ°"å®‰å…¨"ã€"äº‘å®‰å…¨ä¸­å¿ƒ"ã€"SAS" -> "äº‘å®‰å…¨ä¸­å¿ƒ"
   - å…¶ä»–æƒ…å†µé»˜è®¤ä¸º "ECS"
4. å¦‚æœæŸè¡Œä¸åŒ…å«èµ„æºé…ç½®ä¿¡æ¯ï¼Œä¸è¦è¾“å‡º
5. æ•°å­—å¯èƒ½åœ¨ä¸åŒçš„åˆ—ä¸­ï¼Œéœ€è¦æ™ºèƒ½è¯†åˆ«
6. ä¸»æœºæ•°å¯èƒ½ç”¨"å°"ç­‰å•ä½ï¼Œéœ€è¦æå–æ•°å­—
7. è¿”å›çš„JSONå¿…é¡»æ˜¯æœ‰æ•ˆçš„ã€å¯ä»¥ç›´æ¥è§£æçš„æ ¼å¼
"""
        
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹Excelè¡¨æ ¼æ•°æ®ï¼Œæå–æ‰€æœ‰äº‘æœåŠ¡å™¨èµ„æºé…ç½®ä¿¡æ¯ï¼š

{semi_structured_data}

è¯·è¿”å›JSONæ•°ç»„æ ¼å¼çš„ç»“æœã€‚"""
        
        try:
            # è°ƒç”¨Qwen-Plus
            response = dashscope.Generation.call(
                model='qwen-plus',
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': user_prompt}
                ],
                result_format='message',
                temperature=0.1,  # ä½æ¸©åº¦ï¼Œä¿è¯è§£æç¨³å®šæ€§
            )
            
            if response.status_code == HTTPStatus.OK:
                llm_output = response.output.choices[0].message.content
                
                # æå–JSONï¼ˆå¯èƒ½è¢«markdownä»£ç å—åŒ…è£¹ï¼‰
                json_str = llm_output.strip()
                if '```json' in json_str:
                    json_str = json_str.split('```json')[1].split('```')[0].strip()
                elif '```' in json_str:
                    json_str = json_str.split('```')[1].split('```')[0].strip()
                
                # è§£æJSON
                parsed_data = json.loads(json_str)
                
                if not isinstance(parsed_data, list):
                    raise ValueError(f"LLMè¿”å›çš„ä¸æ˜¯æ•°ç»„æ ¼å¼: {type(parsed_data)}")
                
                return parsed_data
            else:
                raise Exception(f"LLM APIè°ƒç”¨å¤±è´¥: {response.code} - {response.message}")
                
        except json.JSONDecodeError as e:
            raise Exception(f"LLMè¿”å›çš„JSONæ ¼å¼æ— æ•ˆ: {e}\nåŸå§‹è¾“å‡º: {llm_output}")
        except Exception as e:
            raise Exception(f"LLMè§£æå¤±è´¥: {e}")
    
    def load_data(self, sheet_name: str = None) -> Iterator[QuotationRequest]:
        """
        ä½¿ç”¨LLMæ™ºèƒ½åŠ è½½å’Œè§£æExcelæ•°æ®
        
        Args:
            sheet_name: æŒ‡å®šè¦è§£æçš„å·¥ä½œè¡¨åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è§£ææ´»åŠ¨å·¥ä½œè¡¨
        
        Yields:
            QuotationRequest: æ ‡å‡†åŒ–çš„è¯·æ±‚å¯¹è±¡
        """
        # Step 1: æå–åŠç»“æ„åŒ–æ•°æ®
        sheet_info = f" (å·¥ä½œè¡¨: {sheet_name})" if sheet_name else ""
        print(f"ğŸ“– æ­£åœ¨è¯»å–Excelæ–‡ä»¶{sheet_info}...")
        semi_structured_data = self._extract_semi_structured_data(sheet_name)
        
        # Step 2: LLMæ™ºèƒ½è§£æ
        print(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨Qwen-Plusæ™ºèƒ½è§£æè¡¨æ ¼{sheet_info}...")
        parsed_data = self._parse_with_llm(semi_structured_data)
        self._parsed_data = parsed_data
        
        print(f"âœ… LLMæˆåŠŸè§£æå‡º {len(parsed_data)} æ¡èµ„æºé…ç½®{sheet_info}")
        
        # Step 3: è½¬æ¢ä¸ºQuotationRequest
        for idx, item in enumerate(parsed_data, 1):
            try:
                cpu_cores = int(item.get('cpu_cores', 0))
                memory_gb = int(item.get('memory_gb', 0))
                storage_gb = int(item.get('storage_gb', 0))
                host_count = int(item.get('host_count', 1))
                description = str(item.get('description', '')).strip()
                row_number = item.get('row_number', idx)
                product_name = str(item.get('product_name', 'ECS')).strip()
                
                # éªŒè¯å¿…éœ€å­—æ®µ
                if cpu_cores <= 0 or memory_gb <= 0:
                    print(f"âš ï¸  è·³è¿‡æ— æ•ˆé…ç½®[{idx}]: CPU={cpu_cores}, MEM={memory_gb}")
                    continue
                
                # æ„é€ å†…å®¹æ–‡æœ¬
                content_text = f"{cpu_cores}C {memory_gb}G"
                if storage_gb > 0:
                    content_text += f" {storage_gb}Gå­˜å‚¨"
                if description:
                    content_text += f" | {description}"
                
                # åœ¨source_idä¸­åŒ…å«å·¥ä½œè¡¨ä¿¡æ¯
                sheet_prefix = f"{sheet_name} - " if sheet_name else ""
                yield QuotationRequest(
                    source_id=f"{sheet_prefix}Row {row_number} (LLM Parsed)",
                    content=content_text,
                    content_type="text",
                    context_notes=description,
                    product_name=product_name,
                    host_count=host_count,
                    cpu_cores=cpu_cores,
                    memory_gb=memory_gb,
                    storage_gb=storage_gb
                )
                
            except (ValueError, KeyError, TypeError) as e:
                print(f"âš ï¸  è§£æé…ç½®é¡¹[{idx}]å¤±è´¥: {e}")
                continue
    
    def get_total_count(self, sheet_name: str = None) -> int:
        """
        è·å–æœ‰æ•ˆæ•°æ®è¡Œæ•°
        
        Args:
            sheet_name: æŒ‡å®šå·¥ä½œè¡¨åç§°
        
        å¦‚æœå·²ç»è§£æè¿‡ï¼Œè¿”å›è§£æç»“æœæ•°é‡ï¼›å¦åˆ™å…ˆè§£æ
        """
        if self._parsed_data is not None:
            return len(self._parsed_data)
        
        # æ‰§è¡Œä¸€æ¬¡å®Œæ•´è§£ææ¥è·å–æ•°é‡
        semi_structured_data = self._extract_semi_structured_data(sheet_name)
        parsed_data = self._parse_with_llm(semi_structured_data)
        self._parsed_data = parsed_data
        
        return len(parsed_data)


# ============================================================================
# Future Extension Point: Image-based Input (Placeholder)
# ============================================================================

class ImageDirLoader(BaseDataLoader):
    """
    å›¾ç‰‡ç›®å½•åŠ è½½å™¨ (æœªæ¥æ‰©å±•)
    
    è®¾è®¡æ€è·¯:
    - éå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    - å°†å›¾ç‰‡è·¯å¾„å°è£…ä¸ºQuotationRequest
    - content_typeè®¾ä¸º"image"
    - ä¸‹æ¸¸Parseræ£€æµ‹åˆ°imageç±»å‹åï¼Œè°ƒç”¨Vision Modelè¿›è¡ŒOCR/ç†è§£
    """
    
    def __init__(self, dir_path: str, supported_formats: tuple = ('.png', '.jpg', '.jpeg')):
        """
        åˆå§‹åŒ–å›¾ç‰‡ç›®å½•åŠ è½½å™¨
        
        Args:
            dir_path: å›¾ç‰‡ç›®å½•è·¯å¾„
            supported_formats: æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        """
        self.dir_path = Path(dir_path)
        self.supported_formats = supported_formats
        
        if not self.dir_path.exists() or not self.dir_path.is_dir():
            raise ValueError(f"Invalid directory path: {dir_path}")
    
    def load_data(self) -> Iterator[QuotationRequest]:
        """
        ä»å›¾ç‰‡ç›®å½•åŠ è½½æ•°æ®
        
        Yields:
            QuotationRequest: å›¾ç‰‡è·¯å¾„å°è£…çš„è¯·æ±‚å¯¹è±¡
        """
        image_files = [
            f for f in self.dir_path.iterdir()
            if f.suffix.lower() in self.supported_formats
        ]
        
        for img_file in sorted(image_files):
            yield QuotationRequest(
                source_id=img_file.name,
                content=str(img_file.absolute()),  # Full path to image
                content_type="image",
                context_notes=""  # Could be extracted from filename or metadata
            )
    
    def get_total_count(self) -> int:
        """è·å–å›¾ç‰‡æ–‡ä»¶æ€»æ•°"""
        return len([
            f for f in self.dir_path.iterdir()
            if f.suffix.lower() in self.supported_formats
        ])
