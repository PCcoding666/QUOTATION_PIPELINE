# -*- coding: utf-8 -*-
"""
Semantic Parser - AI-Powered Text Understanding Module
Phase 5 Enhancement: Real AI Integration with Qwen-Max
- Upgraded from regex rules to Alibaba Cloud DashScope (qwen-max)
- Direct HTTP API calls (no SDK dependency)
- Intelligent workload classification via LLM reasoning
- Token-efficient caching system
"""
import os
import json
import re
import requests
from typing import Dict, Any, Literal
from models import ResourceRequirement

# DashScope API Configuration
DASHSCOPE_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
# Note: API Key is loaded dynamically in parse_with_qwen() to ensure .env is loaded first

# In-Memory Cache for LLM Results (Token Optimization)
_llm_cache: Dict[str, Dict[str, Any]] = {}


def parse_requirement(request: 'QuotationRequest') -> ResourceRequirement:
    """
    è§£ææŠ¥ä»·è¯·æ±‚ä¸ºèµ„æºéœ€æ±‚ (å¤šæ¨¡æ€å…¥å£)
    
    Phase 5: ä½¿ç”¨Qwen-Maxè¿›è¡Œæ™ºèƒ½è§£æ
    - å½“å‰æ”¯æŒ: text (Qwen-Max AIç†è§£)
    - æœªæ¥æ”¯æŒ: image (Qwen-VLå¤šæ¨¡æ€)
    - æœªæ¥æ”¯æŒ: audio (è¯­éŸ³è½¬æ–‡æœ¬ + è§£æ)
    
    Args:
        request: QuotationRequestå¯¹è±¡ (æ¥è‡ªä»»ä½•æ•°æ®æº)
        
    Returns:
        ResourceRequirement: æ ‡å‡†åŒ–çš„èµ„æºéœ€æ±‚å¯¹è±¡
        
    Raises:
        NotImplementedError: å½“content_typeä¸æ”¯æŒæ—¶
    """
    
    # Forward Compatibility Check: Vision Model Integration Point
    if request.content_type == "image":
        raise NotImplementedError(
            "ğŸ”® Qwen-VL integration pending. "
            "Future: Use Qwen-VL to extract specs from screenshots."
        )
    
    # Future Extension Point: Audio/Voice Input
    if request.content_type == "audio":
        raise NotImplementedError(
            "ğŸ¤ Audio transcription + parsing pending. "
            "Future: ASR + Qwen-Max parsing."
        )
    
    # Current Implementation: Text-based AI parsing
    if request.content_type == "text":
        # Combine main content with context notes for richer understanding
        full_text = request.content
        if request.context_notes:
            full_text = f"{full_text} | {request.context_notes}"
        
        return parse_with_qwen(full_text)
    
    # Unsupported content type
    raise ValueError(f"Unsupported content_type: {request.content_type}")


def parse_with_qwen(text: str) -> ResourceRequirement:
    """
    ä½¿ç”¨é˜¿é‡Œäº‘DashScope (qwen-max) è¿›è¡Œæ™ºèƒ½è§£æ
    
    Phase 5æ ¸å¿ƒå‡çº§ï¼šä»è§„åˆ™å¼•æ“åˆ°çœŸæ­£çš„AIç†è§£
    - ä½¿ç”¨HTTP APIç›´æ¥è°ƒç”¨qwen-maxæ¨¡å‹
    - æ™ºèƒ½æ¨ç†å·¥ä½œè´Ÿè½½ç±»å‹
    - ç¼“å­˜æœºåˆ¶ä¼˜åŒ–tokenæ¶ˆè€—
    
    Args:
        text: åŸå§‹éç»“æ„åŒ–æ–‡æœ¬è¾“å…¥
        
    Returns:
        ResourceRequirement: æ ‡å‡†åŒ–çš„èµ„æºéœ€æ±‚å¯¹è±¡
        
    Example:
        >>> parse_with_qwen("16C 64G 1000Gå­˜å‚¨ | å¤‡æ³¨: ç”Ÿäº§ç¯å¢ƒ-å¤šç»´æ•°æ®åº“")
    """
    
    # Step 1: Check cache first (Token optimization)
    if text in _llm_cache:
        print("ğŸ’¾ Cache hit - reusing previous AI analysis")
        cached_result = _llm_cache[text]
        return ResourceRequirement(
            raw_input=text,
            cpu_cores=cached_result["cpu"],
            memory_gb=cached_result["memory"],
            storage_gb=cached_result.get("storage", 0),
            environment="prod",  # Phase 5: No longer classify environment
            workload_type=cached_result["workload_type"]
        )
    
    # Step 2: Call Qwen-Max for AI analysis
    print("ğŸ¤– AI analyzing intent via Qwen-Max...")
    
    system_prompt = """You are an Alibaba Cloud Architect. Analyze the server requirement string.

**Extraction Rules:**
1. Extract CPU (int) and Memory (int).
2. Infer the **Workload Type** based on keywords:
   - "Database", "Redis", "Cache", "Large Memory" -> "memory_intensive"
   - "Algorithm", "Training", "Encoding", "High Freq" -> "compute_intensive"
   - "Web", "App", "Gateway", "General", or Unspecified -> "general_purpose"
3. Ignore environment stages (Dev/Test/Prod).

**Output Format:**
Return strictly valid JSON:
{
  "cpu": 16,
  "memory": 64,
  "workload_type": "memory_intensive" | "compute_intensive" | "general_purpose",
  "reasoning": "Brief reason for classification"
}"""
    
    user_prompt = f"Analyze this requirement: {text}"
    
    try:
        # Load API Key dynamically (to ensure .env is loaded)
        api_key = os.getenv("DASHSCOPE_API_KEY")
        
        if not api_key:
            raise Exception("DASHSCOPE_API_KEY not configured in environment")
        
        # Prepare HTTP request
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "qwen-max",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.1,  # Low temperature for consistent extraction
        }
        
        # Call DashScope API
        response = requests.post(
            DASHSCOPE_API_URL,
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code != 200:
            raise Exception(f"DashScope API Error: {response.status_code} - {response.text}")
        
        # Parse AI response
        response_data = response.json()
        ai_response = response_data["choices"][0]["message"]["content"]
        
        # Extract JSON from response (handle markdown code blocks)
        json_match = re.search(r'```(?:json)?\s*({.*?})\s*```', ai_response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # Try to find raw JSON
            json_match = re.search(r'{.*}', ai_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                raise ValueError(f"Failed to extract JSON from AI response: {ai_response}")
        
        parsed_result = json.loads(json_str)
        
        # Validate required fields
        cpu = int(parsed_result["cpu"])
        memory = int(parsed_result["memory"])
        workload_type = parsed_result["workload_type"]
        
        # Normalize workload_type to match our schema
        workload_map = {
            "memory_intensive": "memory_intensive",
            "compute_intensive": "compute",
            "general_purpose": "general"
        }
        normalized_workload = workload_map.get(workload_type, "general")
        
        # Extract storage using regex fallback (AI may not always provide)
        storage = _extract_storage_gb(text)
        
        # Cache the result
        _llm_cache[text] = {
            "cpu": cpu,
            "memory": memory,
            "storage": storage,
            "workload_type": normalized_workload,
            "reasoning": parsed_result.get("reasoning", "")
        }
        
        print(f"âœ… AI Result: {cpu}C {memory}G -> {normalized_workload} ({parsed_result.get('reasoning', 'N/A')})")
        
        return ResourceRequirement(
            raw_input=text,
            cpu_cores=cpu,
            memory_gb=memory,
            storage_gb=storage,
            environment="prod",  # Phase 5: Simplified - no environment classification
            workload_type=normalized_workload
        )
        
    except Exception as e:
        print(f"âš ï¸ AI parsing failed: {e}. Falling back to regex rules.")
        # Fallback to regex-based parsing
        return _fallback_parse(text)


def _fallback_parse(text: str) -> ResourceRequirement:
    """
    Fallback parsing using regex rules when AI fails
    """
    cpu_cores = _extract_cpu_cores(text)
    memory_gb = _extract_memory_gb(text)
    storage_gb = _extract_storage_gb(text)
    workload_type = _identify_workload_type(text)
    
    return ResourceRequirement(
        raw_input=text,
        cpu_cores=cpu_cores,
        memory_gb=memory_gb,
        storage_gb=storage_gb,
        environment="prod",
        workload_type=workload_type
    )


def _extract_cpu_cores(text: str) -> int:
    """æå–CPUæ ¸å¿ƒæ•° (ä¾‹å¦‚: 16C, 32æ ¸)"""
    # Match patterns like "16C", "32æ ¸", "8 cores"
    patterns = [
        r'(\d+)\s*[Cc](?:\s|$|[^\w])',  # 16C
        r'(\d+)\s*æ ¸',                   # 32æ ¸
        r'(\d+)\s*cores?',               # 8 cores
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return int(match.group(1))
    
    # Default fallback
    return 2


def _extract_memory_gb(text: str) -> int:
    """æå–å†…å­˜å®¹é‡ (ä¾‹å¦‚: 64G, 128GB)"""
    # Match patterns like "64G", "128GB", "32 GB"
    patterns = [
        r'(\d+)\s*[Gg][Bb]?(?:\s|$|[^\w])',  # 64G or 64GB
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return int(match.group(1))
    
    # Default fallback
    return 4


def _extract_storage_gb(text: str) -> int:
    """æå–å­˜å‚¨å®¹é‡ (ä¾‹å¦‚: 1000Gå­˜å‚¨, 500GBç£ç›˜)"""
    # Match patterns like "1000Gå­˜å‚¨", "500GB"
    patterns = [
        r'(\d+)\s*[Gg][Bb]?\s*å­˜å‚¨',     # 1000Gå­˜å‚¨
        r'å­˜å‚¨\s*[:\:ï¼š]?\s*(\d+)\s*[Gg]', # å­˜å‚¨: 500G
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return int(match.group(1))
    
    # Default fallback
    return 0


def _identify_environment(text: str) -> Literal["dev", "prod", "test"]:
    """
    è¯†åˆ«ç¯å¢ƒç±»å‹
    å…³é”®è¯æ˜ å°„:
    - ç”Ÿäº§/production/prod -> prod
    - å¼€å‘/development/dev -> dev
    - æµ‹è¯•/test/staging -> test
    """
    text_lower = text.lower()
    
    # Production environment keywords
    if any(keyword in text for keyword in ["ç”Ÿäº§", "æ­£å¼"]):
        return "prod"
    if any(keyword in text_lower for keyword in ["production", "prod"]):
        return "prod"
    
    # Development environment keywords
    if any(keyword in text for keyword in ["å¼€å‘", "ç ”å‘"]):
        return "dev"
    if any(keyword in text_lower for keyword in ["development", "dev"]):
        return "dev"
    
    # Test environment keywords
    if any(keyword in text for keyword in ["æµ‹è¯•", "é¢„å‘", "ç°åº¦"]):
        return "test"
    if any(keyword in text_lower for keyword in ["test", "staging", "uat"]):
        return "test"
    
    # Default to dev
    return "dev"


def _identify_workload_type(text: str) -> Literal["general", "compute", "memory_intensive"]:
    """
    è¯†åˆ«å·¥ä½œè´Ÿè½½ç±»å‹ (AIæ ¸å¿ƒé€»è¾‘)
    å…³é”®è¯æ˜ å°„:
    - æ•°æ®åº“/ç¼“å­˜/Redis -> memory_intensive (å†…å­˜å¯†é›†å‹)
    - ç®—æ³•/AI/è®­ç»ƒ/è®¡ç®— -> compute (è®¡ç®—å¯†é›†å‹)
    - ä¸­é—´ä»¶/Web/API -> general (é€šç”¨å‹)
    """
    text_lower = text.lower()
    
    # Memory-intensive workload keywords
    memory_keywords = ["æ•°æ®åº“", "ç¼“å­˜", "redis", "memcache", "mysql", "oracle", "postgresql", "mongo"]
    if any(keyword in text_lower for keyword in memory_keywords):
        return "memory_intensive"
    
    # Compute-intensive workload keywords
    compute_keywords = ["ç®—æ³•", "ai", "è®­ç»ƒ", "è®¡ç®—", "æ·±åº¦å­¦ä¹ ", "machine learning", "gpu", "ç§‘å­¦è®¡ç®—"]
    if any(keyword in text_lower for keyword in compute_keywords):
        return "compute"
    
    # General workload keywords
    general_keywords = ["ä¸­é—´ä»¶", "web", "api", "ç½‘å…³", "nginx", "tomcat", "åº”ç”¨æœåŠ¡"]
    if any(keyword in text_lower for keyword in general_keywords):
        return "general"
    
    # Default to general
    return "general"
