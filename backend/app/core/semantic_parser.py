# -*- coding: utf-8 -*-
"""
Semantic Parser - AI-Powered Text Understanding Module
Phase 5 Enhancement: Real AI Integration with Qwen-Max
- Upgraded from regex rules to Alibaba Cloud DashScope (qwen-max)
- Direct HTTP API calls (no SDK dependency)
- Intelligent workload classification via LLM reasoning
- Token-efficient caching system
"""
import os
import json
import re
import requests
from typing import Dict, Any, Literal
from app.models.domain import ResourceRequirement

# DashScope API Configuration
DASHSCOPE_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
# Note: API Key is loaded dynamically in parse_with_qwen() to ensure .env is loaded first

# In-Memory Cache for LLM Results (Token Optimization)
_llm_cache: Dict[str, Dict[str, Any]] = {}

# PolarDBç›¸å…³å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºæ£€æµ‹é ECS åœºæ™¯ï¼‰
# ç­–ç•¥ï¼šæå…¶ä¸¥æ ¼ï¼Œå¿…é¡»åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶æ‰è¯†åˆ«ä¸º PolarDBï¼š
#   1. æåˆ° PolarDB äº§å“åç§°
#   2. æåˆ° PolarDB çš„å‡†ç¡®è§„æ ¼å‹å·ï¼ˆå¦‚ polar.mysql.x4.largeï¼‰
# å¦åˆ™ï¼Œå³ä½¿æåˆ° PolarDBï¼Œä¹Ÿè§†ä¸ºåœ¨ ECS ä¸Šéƒ¨ç½² PolarDB åº”ç”¨ï¼Œè¯†åˆ«ä¸º ECS
POLARDB_KEYWORDS = [
    "polardb", "polar db", "polar-db", "PolarDB", "POLARDB",
]


def _is_polardb_request(text: str) -> bool:
    """
    æ£€æµ‹è¾“å…¥æ–‡æœ¬æ˜¯å¦ä¸º PolarDB äº§å“è§„æ ¼è¯·æ±‚
    
    ç­–ç•¥ï¼šæå…¶ä¸¥æ ¼ï¼Œå¿…é¡»åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š
    1. æåˆ° "PolarDB" äº§å“åç§°
    2. æåˆ° PolarDB çš„å‡†ç¡®è§„æ ¼å‹å·ï¼ˆå¦‚ polar.mysql.x4.largeã€polar.pg.x8.mediumï¼‰
    
    å¦åˆ™ï¼Œå³ä½¿æåˆ° PolarDBï¼Œä¹Ÿè§†ä¸ºåœ¨ ECS ä¸Šéƒ¨ç½² PolarDB åº”ç”¨ï¼Œè¯†åˆ«ä¸º ECS
    
    ç¤ºä¾‹ï¼š
    - "16C 64G | PolarDBæ•°æ®åº“" â†’ ECSï¼ˆåªæ˜¯æè¿°ï¼Œæ²¡æœ‰å‡†ç¡®è§„æ ¼ï¼‰
    - "polar.mysql.x4.large" â†’ PolarDBï¼ˆæœ‰å‡†ç¡®è§„æ ¼ï¼‰
    - "PolarDB polar.mysql.x4.large" â†’ PolarDBï¼ˆåŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼‰
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        bool: å¦‚æœæ˜¯ PolarDB è§„æ ¼è¯·æ±‚è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    text_lower = text.lower()
    
    # æ¡ä»¶1ï¼šæ£€æŸ¥æ˜¯å¦æåˆ° PolarDB äº§å“åç§°
    has_polardb_keyword = False
    polardb_keywords = ["polardb", "polar db", "polar-db"]
    
    for keyword in polardb_keywords:
        if keyword in text_lower:
            has_polardb_keyword = True
            break
    
    # æ¡ä»¶2ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å« PolarDB çš„å‡†ç¡®è§„æ ¼å‹å·
    # PolarDB è§„æ ¼æ ¼å¼ï¼špolar.{mysql|pg|o}.{xæ•°å­—}.{è§„æ ¼}
    # ä¾‹å¦‚ï¼špolar.mysql.x4.large, polar.pg.x8.medium, polar.o.x4.xlarge
    import re
    polardb_spec_pattern = r'polar\.(mysql|pg|o)\.x\d+\.(small|medium|large|xlarge|2xlarge|4xlarge|8xlarge|12xlarge|16xlarge)'
    has_polardb_spec = bool(re.search(polardb_spec_pattern, text_lower))
    
    # å¿…é¡»åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶
    # æˆ–è€…å•ç‹¬å‡ºç°è§„æ ¼å‹å·ï¼ˆè§„æ ¼æœ¬èº«å°±åŒ…å« polar å‰ç¼€ï¼‰
    return (has_polardb_keyword and has_polardb_spec) or has_polardb_spec


def _get_ecs_enhanced_system_prompt(is_ecs_scenario: bool) -> str:
    """
    æ ¹æ®åœºæ™¯ç±»å‹ç”Ÿæˆå¢å¼ºçš„ç³»ç»Ÿæç¤ºè¯
    
    Args:
        is_ecs_scenario: æ˜¯å¦ä¸º ECS å®ä¾‹éƒ¨ç½²åœºæ™¯
        
    Returns:
        str: ç³»ç»Ÿæç¤ºè¯
    """
    base_prompt = """You are an Alibaba Cloud Architect. Analyze the server requirement string.

**Extraction Rules:**
1. Extract CPU (int) and Memory (int).
2. Infer the **Workload Type** based on keywords:
   - "Database", "Redis", "Cache", "Large Memory" -> "memory_intensive"
   - "Algorithm", "Training", "Encoding", "High Freq" -> "compute_intensive"
   - "Web", "App", "Gateway", "General", or Unspecified -> "general_purpose"
3. Ignore environment stages (Dev/Test/Prod).

**Output Format:**
Return strictly valid JSON:
{
  "cpu": 16,
  "memory": 64,
  "workload_type": "memory_intensive" | "compute_intensive" | "general_purpose",
  "reasoning": "Brief reason for classification"
}"""
    
    if is_ecs_scenario:
        # ECS åœºæ™¯å¢å¼ºæç¤º
        ecs_enhancement = """

**IMPORTANT - ECS Instance Scenario:**
This request is for an **ECS (Elastic Compute Service) instance deployment**.
- The output should be interpreted as ECS virtual machine specifications.
- Focus on CPU cores, memory size, and workload characteristics for ECS SKU matching.
- Do NOT interpret this as a managed database service (like PolarDB, RDS, etc.).
- The recommended SKU will be used for ECS instance type selection (e.g., ecs.g9i.xlarge)."""
        return base_prompt + ecs_enhancement
    
    return base_prompt


def parse_requirement(request: 'QuotationRequest') -> ResourceRequirement:
    """
    è§£ææŠ¥ä»·è¯·æ±‚ä¸ºèµ„æºéœ€æ±‚ (å¤šæ¨¡æ€å…¥å£)
    
    Phase 5: ä½¿ç”¨Qwen-Maxè¿›è¡Œæ™ºèƒ½è§£æ
    - å½“å‰æ”¯æŒ: text (Qwen-Max AIç†è§£)
    - æœªæ¥æ”¯æŒ: image (Qwen-VLå¤šæ¨¡æ€)
    - æœªæ¥æ”¯æŒ: audio (è¯­éŸ³è½¬æ–‡æœ¬ + è§£æ)
    
    Args:
        request: QuotationRequestå¯¹è±¡ (æ¥è‡ªä»»ä½•æ•°æ®æº)
        
    Returns:
        ResourceRequirement: æ ‡å‡†åŒ–çš„èµ„æºéœ€æ±‚å¯¹è±¡
        
    Raises:
        NotImplementedError: å½“content_typeä¸æ”¯æŒæ—¶
    """
    
    # Forward Compatibility Check: Vision Model Integration Point
    if request.content_type == "image":
        raise NotImplementedError(
            "ğŸ”® Qwen-VL integration pending. "
            "Future: Use Qwen-VL to extract specs from screenshots."
        )
    
    # Future Extension Point: Audio/Voice Input
    if request.content_type == "audio":
        raise NotImplementedError(
            "ğŸ¤ Audio transcription + parsing pending. "
            "Future: ASR + Qwen-Max parsing."
        )
    
    # Current Implementation: Text-based AI parsing
    if request.content_type == "text":
        # Combine main content with context notes for richer understanding
        full_text = request.content
        if request.context_notes:
            full_text = f"{full_text} | {request.context_notes}"
        
        return parse_with_qwen(full_text)
    
    # Unsupported content type
    raise ValueError(f"Unsupported content_type: {request.content_type}")


def parse_with_qwen(text: str) -> ResourceRequirement:
    """
    ä½¿ç”¨é˜¿é‡Œäº‘DashScope (qwen-max) è¿›è¡Œæ™ºèƒ½è§£æ
    
    Phase 5æ ¸å¿ƒå‡çº§ï¼šä»è§„åˆ™å¼•æ“åˆ°çœŸæ­£çš„AIç†è§£
    - ä½¿ç”¨HTTP APIç›´æ¥è°ƒç”¨qwen-maxæ¨¡å‹
    - æ™ºèƒ½æ¨ç†å·¥ä½œè´Ÿè½½ç±»å‹
    - ç¼“å­˜æœºåˆ¶ä¼˜åŒ–tokenæ¶ˆè€—
    
    å¢å¼ºåŠŸèƒ½ï¼š
    - è‹¥è¾“å…¥æ–‡æœ¬ä¸­æœªæ˜ç¡®æåŠ PolarDB ç­‰å…³é”®è¯ï¼Œé»˜è®¤ä¸º ECS å®ä¾‹éƒ¨ç½²åœºæ™¯
    - å¯¹ ECS åœºæ™¯å¢å¼ºæç¤ºï¼Œç¡®ä¿è¯­ä¹‰è§£æä¸º ECS å®ä¾‹éœ€æ±‚
    
    Args:
        text: åŸå§‹éç»“æ„åŒ–æ–‡æœ¬è¾“å…¥
        
    Returns:
        ResourceRequirement: æ ‡å‡†åŒ–çš„èµ„æºéœ€æ±‚å¯¹è±¡
        
    Example:
        >>> parse_with_qwen("16C 64G 1000Gå­˜å‚¨ | å¤‡æ³¨: ç”Ÿäº§ç¯å¢ƒ-å¤šç»´æ•°æ®åº“")
    """
    
    # Step 1: Check cache first (Token optimization)
    if text in _llm_cache:
        print("ğŸ’¾ Cache hit - reusing previous AI analysis")
        cached_result = _llm_cache[text]
        return ResourceRequirement(
            raw_input=text,
            cpu_cores=cached_result["cpu"],
            memory_gb=cached_result["memory"],
            storage_gb=cached_result.get("storage", 0),
            environment="prod",  # Phase 5: No longer classify environment
            workload_type=cached_result["workload_type"]
        )
    
    # Step 2: æ£€æµ‹æ˜¯å¦ä¸º ECS åœºæ™¯ï¼ˆé»˜è®¤ä¸º ECSï¼Œé™¤éæ˜ç¡®æåŠ PolarDB ç­‰å…³é”®è¯ï¼‰
    is_ecs_scenario = not _is_polardb_request(text)
    
    if is_ecs_scenario:
        print("ğŸ’» ECS Instance Scenario detected - applying ECS-specific parsing")
    else:
        print("ğŸ—„ï¸  PolarDB/RDS Scenario detected - using standard parsing")
    
    # Step 3: Call Qwen-Max for AI analysis with enhanced prompt
    print("ğŸ¤– AI analyzing intent via Qwen-Max...")
    
    # ä½¿ç”¨å¢å¼ºçš„ç³»ç»Ÿæç¤ºè¯
    system_prompt = _get_ecs_enhanced_system_prompt(is_ecs_scenario)
    
    user_prompt = f"Analyze this requirement: {text}"
    
    try:
        # Load API Key dynamically (to ensure .env is loaded)
        api_key = os.getenv("DASHSCOPE_API_KEY")
        
        if not api_key:
            raise Exception("DASHSCOPE_API_KEY not configured in environment")
        
        # Prepare HTTP request
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "qwen-max",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.1,  # Low temperature for consistent extraction
        }
        
        # Call DashScope API
        response = requests.post(
            DASHSCOPE_API_URL,
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code != 200:
            raise Exception(f"DashScope API Error: {response.status_code} - {response.text}")
        
        # Parse AI response
        response_data = response.json()
        ai_response = response_data["choices"][0]["message"]["content"]
        
        # Extract JSON from response (handle markdown code blocks)
        json_match = re.search(r'```(?:json)?\s*({.*?})\s*```', ai_response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # Try to find raw JSON
            json_match = re.search(r'{.*}', ai_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                raise ValueError(f"Failed to extract JSON from AI response: {ai_response}")
        
        parsed_result = json.loads(json_str)
        
        # Validate required fields
        cpu = int(parsed_result["cpu"])
        memory = int(parsed_result["memory"])
        workload_type = parsed_result["workload_type"]
        
        # Normalize workload_type to match our schema
        workload_map = {
            "memory_intensive": "memory_intensive",
            "compute_intensive": "compute",
            "general_purpose": "general"
        }
        normalized_workload = workload_map.get(workload_type, "general")
        
        # Extract storage using regex fallback (AI may not always provide)
        storage = _extract_storage_gb(text)
        
        # Cache the result
        _llm_cache[text] = {
            "cpu": cpu,
            "memory": memory,
            "storage": storage,
            "workload_type": normalized_workload,
            "reasoning": parsed_result.get("reasoning", "")
        }
        
        print(f"âœ… AI Result: {cpu}C {memory}G -> {normalized_workload} ({parsed_result.get('reasoning', 'N/A')})")
        
        return ResourceRequirement(
            raw_input=text,
            cpu_cores=cpu,
            memory_gb=memory,
            storage_gb=storage,
            environment="prod",  # Phase 5: Simplified - no environment classification
            workload_type=normalized_workload
        )
        
    except Exception as e:
        print(f"âš ï¸ AI parsing failed: {e}. Falling back to regex rules.")
        # Fallback to regex-based parsing
        return _fallback_parse(text)


def _fallback_parse(text: str) -> ResourceRequirement:
    """
    Fallback parsing using regex rules when AI fails
    """
    cpu_cores = _extract_cpu_cores(text)
    memory_gb = _extract_memory_gb(text)
    storage_gb = _extract_storage_gb(text)
    workload_type = _identify_workload_type(text)
    
    return ResourceRequirement(
        raw_input=text,
        cpu_cores=cpu_cores,
        memory_gb=memory_gb,
        storage_gb=storage_gb,
        environment="prod",
        workload_type=workload_type
    )


def _extract_cpu_cores(text: str) -> int:
    """æå–CPUæ ¸å¿ƒæ•° (ä¾‹å¦‚: 16C, 32æ ¸)"""
    # Match patterns like "16C", "32æ ¸", "8 cores"
    patterns = [
        r'(\d+)\s*[Cc](?:\s|$|[^\w])',  # 16C
        r'(\d+)\s*æ ¸',                   # 32æ ¸
        r'(\d+)\s*cores?',               # 8 cores
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return int(match.group(1))
    
    # Default fallback
    return 2


def _extract_memory_gb(text: str) -> int:
    """æå–å†…å­˜å®¹é‡ (ä¾‹å¦‚: 64G, 128GB)"""
    # Match patterns like "64G", "128GB", "32 GB"
    patterns = [
        r'(\d+)\s*[Gg][Bb]?(?:\s|$|[^\w])',  # 64G or 64GB
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return int(match.group(1))
    
    # Default fallback
    return 4


def _extract_storage_gb(text: str) -> int:
    """æå–å­˜å‚¨å®¹é‡ (ä¾‹å¦‚: 1000Gå­˜å‚¨, 500GBç£ç›˜)"""
    # Match patterns like "1000Gå­˜å‚¨", "500GB"
    patterns = [
        r'(\d+)\s*[Gg][Bb]?\s*å­˜å‚¨',     # 1000Gå­˜å‚¨
        r'å­˜å‚¨\s*[:\:ï¼š]?\s*(\d+)\s*[Gg]', # å­˜å‚¨: 500G
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return int(match.group(1))
    
    # Default fallback
    return 0


def _identify_environment(text: str) -> Literal["dev", "prod", "test"]:
    """
    è¯†åˆ«ç¯å¢ƒç±»å‹
    å…³é”®è¯æ˜ å°„:
    - ç”Ÿäº§/production/prod -> prod
    - å¼€å‘/development/dev -> dev
    - æµ‹è¯•/test/staging -> test
    """
    text_lower = text.lower()
    
    # Production environment keywords
    if any(keyword in text for keyword in ["ç”Ÿäº§", "æ­£å¼"]):
        return "prod"
    if any(keyword in text_lower for keyword in ["production", "prod"]):
        return "prod"
    
    # Development environment keywords
    if any(keyword in text for keyword in ["å¼€å‘", "ç ”å‘"]):
        return "dev"
    if any(keyword in text_lower for keyword in ["development", "dev"]):
        return "dev"
    
    # Test environment keywords
    if any(keyword in text for keyword in ["æµ‹è¯•", "é¢„å‘", "ç°åº¦"]):
        return "test"
    if any(keyword in text_lower for keyword in ["test", "staging", "uat"]):
        return "test"
    
    # Default to dev
    return "dev"


def _identify_workload_type(text: str) -> Literal["general", "compute", "memory_intensive"]:
    """
    è¯†åˆ«å·¥ä½œè´Ÿè½½ç±»å‹ (AIæ ¸å¿ƒé€»è¾‘)
    å…³é”®è¯æ˜ å°„:
    - æ•°æ®åº“/ç¼“å­˜/Redis -> memory_intensive (å†…å­˜å¯†é›†å‹)
    - ç®—æ³•/AI/è®­ç»ƒ/è®¡ç®— -> compute (è®¡ç®—å¯†é›†å‹)
    - ä¸­é—´ä»¶/Web/API -> general (é€šç”¨å‹)
    """
    text_lower = text.lower()
    
    # Memory-intensive workload keywords
    memory_keywords = ["æ•°æ®åº“", "ç¼“å­˜", "redis", "memcache", "mysql", "oracle", "postgresql", "mongo"]
    if any(keyword in text_lower for keyword in memory_keywords):
        return "memory_intensive"
    
    # Compute-intensive workload keywords
    compute_keywords = ["ç®—æ³•", "ai", "è®­ç»ƒ", "è®¡ç®—", "æ·±åº¦å­¦ä¹ ", "machine learning", "gpu", "ç§‘å­¦è®¡ç®—"]
    if any(keyword in text_lower for keyword in compute_keywords):
        return "compute"
    
    # General workload keywords
    general_keywords = ["ä¸­é—´ä»¶", "web", "api", "ç½‘å…³", "nginx", "tomcat", "åº”ç”¨æœåŠ¡"]
    if any(keyword in text_lower for keyword in general_keywords):
        return "general"
    
    # Default to general
    return "general"
